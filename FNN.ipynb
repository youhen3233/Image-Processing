{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WwcHOWc4aDXxpCw180Oh4-gEda7BkYAs",
      "authorship_tag": "ABX9TyM+S606bhn4A5qWQ/r6j9Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youhen3233/Image-Processing/blob/main/FNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4w3hQgtyeQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "246a8daa-440a-4500-df3f-b9a28c7d5658"
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/train.npz')\n",
        "test_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/test.npz')\n",
        "\n",
        "\n",
        "# Data Pre_process==========================\n",
        "x_train = train_data['image']\n",
        "y_train = train_data['label']\n",
        "\n",
        "x_test = test_data['image']\n",
        "y_test = test_data['label']\n",
        "\n",
        "one_hot_list = [ [1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]]\n",
        "\n",
        "x_train_row = x_train.reshape(x_train.shape[0] , x_train.shape[1]*x_train.shape[2])\n",
        "y_train_one_hot = []\n",
        "for i in range(len(y_train)):\n",
        "  y_train_one_hot.append( one_hot_list[y_train[i]] )\n",
        "y_train_lab = np.array(y_train_one_hot) \n",
        "\n",
        "x_test_row = x_test.reshape(x_test.shape[0] , x_test.shape[1]*x_test.shape[2])\n",
        "y_test_one_hot = []\n",
        "for i in range(len(y_test)):\n",
        "  y_test_one_hot.append( one_hot_list[y_test[i]] )\n",
        "y_test_lab = np.array(y_test_one_hot) \n",
        "\n",
        "x_train_row = x_train_row / 255\n",
        "x_test_row = x_test_row / 255\n",
        "\n",
        "\n",
        "print(y_train_lab)\n",
        "\n",
        "# Set up Model=============================\n",
        "nn_struct = [\n",
        "    {\"input_dim\": 1024, \"output_dim\": 256, \"act\": \"relu\"},\n",
        "    {\"input_dim\": 256, \"output_dim\": 64, \"act\": \"relu\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 6, \"act\": \"softmax\"},\n",
        "]\n",
        "\n",
        "\n",
        "#===functions===============\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def softmax(Z):\n",
        "    c = np.max(Z)\n",
        "    exp_Z = np.exp(Z - c)\n",
        "    sum_exp_Z = np.sum(exp_Z)\n",
        "    return exp_Z / sum_exp_Z\n",
        "\n",
        "\n",
        "def softmax_backward(dA ,Z):\n",
        "    dZ = Z - dA\n",
        "    return dZ\n",
        "\n",
        "\n",
        "\n",
        "#Loss and accuracy===============\n",
        "\n",
        "def convert_prob_into_class(probs):   #one hot prob\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    #print(Y)\n",
        "    #print(Y_hat)\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    #print(Y_hat_)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()\n",
        "\n",
        "\n",
        "def cross_entropy(Y_hat, Y):\n",
        "    delta = 1e-7\n",
        "    return -np.sum(Y * np.log(Y_hat + delta))\n",
        "\n",
        "\"\"\"\n",
        "def cross_entropy(Y_hat, Y):\n",
        "    epsilon =1e-12\n",
        "    Y_hat = np.clip(Y_hat, epsilon, 1.-epsilon)\n",
        "    N = Y_hat.shape[0]\n",
        "    ce = - np.sum(Y * np.log(Y_hat)) \n",
        "    return ce\n",
        "\"\"\"\n",
        "#=======Update new param\n",
        "\n",
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "    for layer_idx, layer in enumerate(nn_architecture):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]\n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return params_values\n",
        "\n",
        "\n",
        "#=====set up model=====\n",
        "def init_layers(nn_architecture, seed=99):\n",
        "    np.random.seed(seed)\n",
        "    W_B_DIC = {}   #dict for Weight and bias\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        input_size = layer[\"input_dim\"]\n",
        "        output_size = layer[\"output_dim\"]\n",
        "\n",
        "        W_B_DIC['W' + str(idx)] = np.random.randn( output_size, input_size) / np.sqrt(4)\n",
        "        W_B_DIC['b' + str(idx)] = np.random.randn( output_size, 1) / np.sqrt(4)        \n",
        "\n",
        "    return W_B_DIC\n",
        "\n",
        "\n",
        "def for_prop(X, W_B_DIC, nn_architecture):\n",
        "    memory = {}\n",
        "    A_curr = X.T\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        A_prev = A_curr\n",
        "\n",
        "        act_curr = layer[\"act\"]\n",
        "        W_curr = W_B_DIC[\"W\" + str(idx)]\n",
        "        b_curr = W_B_DIC[\"b\" + str(idx)]\n",
        "        #print(\"X_CURR\",A_prev.shape)\n",
        "        #print(\"W_CURR\",W_curr.shape)\n",
        "        #print(\"B_CURR\",b_curr.shape)\n",
        "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "        #print(\"Z_CURR\",Z_curr.shape)\n",
        "        if act_curr == \"relu\":\n",
        "            activation_func = relu\n",
        "        elif act_curr == \"softmax\":\n",
        "            activation_func = softmax\n",
        "\n",
        "        A_curr = activation_func(Z_curr)\n",
        "\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(idx)] = Z_curr\n",
        "    #print(\"final_output:\",A_curr.shape)\n",
        "    return A_curr, memory    \n",
        "\n",
        "\n",
        "def backward_propagation(Y_hat, Y, memory, W_B_DIC, nn_architecture):\n",
        "    grads_values = {}\n",
        "    Y = Y.T\n",
        "    dA_prev = Y\n",
        "\n",
        "\n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        activ_function_curr = layer[\"act\"]\n",
        "\n",
        "        dA_curr = dA_prev\n",
        "\n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_prev)]\n",
        "        W_curr = W_B_DIC[\"W\" + str(layer_idx_prev)]\n",
        "        if activ_function_curr == \"relu\":\n",
        "            backward_activation_func = relu_backward\n",
        "        elif activ_function_curr == \"softmax\":    \n",
        "            backward_activation_func = softmax_backward\n",
        "            \n",
        "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "        dW_curr = np.dot(dZ_curr, A_prev.T) \n",
        "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) \n",
        "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "        grads_values[\"dW\" + str(layer_idx_prev)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_prev)] = db_curr\n",
        "\n",
        "    return grads_values\n",
        "\n",
        "\n",
        "\n",
        "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
        "    W_B_DIC = init_layers(nn_architecture,55)\n",
        "    loss_list = []\n",
        "    accuracy_list = []\n",
        "    loss_dict = {}\n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = for_prop( X, W_B_DIC, nn_architecture)   #Y_hat = Colm major(估算值)\n",
        "        print(y_train_lab)\n",
        "        print(Y_hat)\n",
        "        loss = cross_entropy(Y_hat.T, Y)\n",
        "        loss_list.append(loss)\n",
        "        accuracy = get_accuracy_value(Y_hat.T, Y)\n",
        "        accuracy_list.append(accuracy)\n",
        "        loss_dict[i]=loss\n",
        "        print(\"epoch \" ,i,\" loss :\",loss ,\" accuracy :\", accuracy)\n",
        "        #print(i)\n",
        "        #print(loss)\n",
        "        #if i % 100 == 0:\n",
        "        #    print(\"epoch \" ,i,\" loss :\",loss)\n",
        "\n",
        "        grads_values = backward_propagation(Y_hat, Y, cashe, W_B_DIC, nn_architecture)\n",
        "        W_B_DIC = update(W_B_DIC, grads_values, nn_architecture, learning_rate)\n",
        "\n",
        "    return W_B_DIC, loss_list, accuracy_list, loss_dict\n",
        "\n",
        "\n",
        "\n",
        "zz,lost,acc,loss_dict = train(x_train_row, y_train_lab, nn_struct, 200, 0.001)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[2.29397761e-142 3.46833946e-207 1.50855730e-227 ... 2.28233509e-211\n",
            "  8.15040121e-145 2.84228808e-200]\n",
            " [6.42103883e-225 2.69209875e-236 2.78168437e-228 ... 2.83138299e-219\n",
            "  5.20093629e-223 1.92277736e-265]\n",
            " [6.65553434e-225 9.69671666e-237 1.16078271e-237 ... 2.71486336e-197\n",
            "  2.71271594e-270 5.02709138e-245]\n",
            " [0.00000000e+000 6.31053224e-274 4.42929851e-320 ... 9.93000375e-270\n",
            "  0.00000000e+000 1.74961995e-308]\n",
            " [7.96106403e-209 1.62560960e-236 2.85477698e-253 ... 2.72276952e-215\n",
            "  1.22311926e-215 2.61947529e-252]\n",
            " [6.44420768e-253 3.03606187e-201 1.52720864e-229 ... 8.82795436e-199\n",
            "  1.13929397e-243 2.04551778e-216]]\n",
            "epoch  0  loss : 822022.8781988743  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "epoch  1  loss : 822006.7601031234  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  2  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  3  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  4  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  5  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  6  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  7  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  8  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  9  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  10  loss : 777112.2630833209  accuracy : 0.0\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " ...\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0]]\n",
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.96078431e-05 1.96078431e-05 1.96078431e-05 ... 1.96078431e-05\n",
            "  1.96078431e-05 1.96078431e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "epoch  11  loss : 777112.2630833209  accuracy : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-29a4b75aaaf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mzz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-29a4b75aaaf7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m#    print(\"epoch \" ,i,\" loss :\",loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mgrads_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcashe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_B_DIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mW_B_DIC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_B_DIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-29a4b75aaaf7>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(Y_hat, Y, memory, W_B_DIC, nn_architecture)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mdZ_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_activation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mdW_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mdb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_curr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBl2PBIm3nej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8ec17e-6dd0-48ae-d6b9-cc7d47d8a268"
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/train.npz')\n",
        "test_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/test.npz')\n",
        "\n",
        "\n",
        "# Data Pre_process==========================\n",
        "x_train = train_data['image']\n",
        "y_train = train_data['label']\n",
        "\n",
        "x_test = test_data['image']\n",
        "y_test = test_data['label']\n",
        "\n",
        "\n",
        "one_hot_list = [ [1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]]\n",
        "\n",
        "x_train_row = x_train.reshape(x_train.shape[0] , x_train.shape[1]*x_train.shape[2])\n",
        "y_train_one_hot = []\n",
        "for i in range(len(y_train)):\n",
        "  y_train_one_hot.append( one_hot_list[y_train[i]] )\n",
        "y_train_lab = np.array(y_train_one_hot) \n",
        "y_train_lab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojvQPXt-cGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7104a526-e12f-4d3a-9a08-ae1afb5bda59"
      },
      "source": [
        "nn_struct = [\n",
        "    {\"input_dim\": 1024, \"output_dim\": 256, \"act\": \"relu\"},\n",
        "    {\"input_dim\": 256, \"output_dim\": 64, \"act\": \"relu\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 6, \"act\": \"softmax\"},\n",
        "]\n",
        "\n",
        "\n",
        "for layer_idx_prev, layer in reversed(list(enumerate(nn_struct))):\n",
        "\n",
        "  print(layer_idx_prev)\n",
        "  print(layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "{'input_dim': 64, 'output_dim': 6, 'act': 'softmax'}\n",
            "1\n",
            "{'input_dim': 256, 'output_dim': 64, 'act': 'relu'}\n",
            "0\n",
            "{'input_dim': 1024, 'output_dim': 256, 'act': 'relu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33fz75S_2Im_"
      },
      "source": [
        "nn_architecture = [\n",
        "    {\"input_dim\": 1024, \"output_dim\": 256, \"act\": \"sigmoid\"},\n",
        "    {\"input_dim\": 256, \"output_dim\": 64, \"act\": \"sigmoid\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 6, \"act\": \"sigmoid\"},\n",
        "]\n",
        "\n",
        "def init_layers(nn_architecture, seed=99):\n",
        "    np.random.seed(seed)\n",
        "    W_B_DIC = {}   #dict for Weight and bias\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx \n",
        "        input_size = layer[\"input_dim\"]\n",
        "        output_size = layer[\"output_dim\"]\n",
        "\n",
        "        W_B_DIC['W' + str(layer_idx)] = np.random.randn( output_size, input_size) / np.sqrt(4)\n",
        "        W_B_DIC['b' + str(layer_idx)] = np.random.randn( output_size, 1) / np.sqrt(4)        \n",
        "\n",
        "    return W_B_DIC\n",
        "\n",
        "ccc = init_layers(nn_architecture, 87) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_yErYUk2Ir_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEWIvBxZ3qpp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYFSgHMKEe32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "ad5590c4-2da0-43e0-e41c-5f9cac5dccd5"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nn_architecture = [\n",
        "    {\"input_dim\": 1024, \"output_dim\": 256, \"act\": \"sigmoid\"},\n",
        "    {\"input_dim\": 256, \"output_dim\": 64, \"act\": \"sigmoid\"},\n",
        "    {\"input_dim\": 64, \"output_dim\": 6, \"act\": \"sigmoid\"},\n",
        "]\n",
        "\n",
        "def init_layers(nn_architecture, seed=99):\n",
        "    np.random.seed(seed)\n",
        "    params_values = {}\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "\n",
        "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, layer_input_size) / np.sqrt(4)\n",
        "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, 1) / np.sqrt(4)\n",
        "\n",
        "    return params_values\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def softmax_backward(dA, Z):\n",
        "    \n",
        "    return Z\n",
        "\n",
        "\n",
        "def linear(Z):\n",
        "    return Z\n",
        "\n",
        "def linear_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    return dZ\n",
        "\n",
        "def forward_propagation(X, params_values, nn_architecture):\n",
        "    memory = {}\n",
        "    A_curr = X\n",
        "\n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        A_prev = A_curr\n",
        "\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        \n",
        "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "        \n",
        "        if activ_function_curr == \"relu\":\n",
        "            activation_func = relu\n",
        "        elif activ_function_curr == \"sigmoid\":\n",
        "            activation_func = sigmoid\n",
        "        elif activ_function_curr == \"linear\":\n",
        "            activation_func = linear\n",
        "\n",
        "        A_curr = activation_func(Z_curr)\n",
        "\n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "\n",
        "    return A_curr, memory\n",
        "\n",
        "def get_loss_value(Y_hat, Y): #Binary Cross-entropy\n",
        "    m = Y_hat.shape[1]\n",
        "    loss = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
        "    return np.squeeze(loss)\n",
        "\n",
        "def get_accuracy_value(Y_hat, Y):\n",
        "    print(Y_hat)\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    print(Y_hat_)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()\n",
        "\n",
        "\n",
        "def convert_prob_into_class(probs):\n",
        "    probs_ = np.copy(probs)\n",
        "    probs_[probs_ > 0.5] = 1\n",
        "    probs_[probs_ <= 0.5] = 0\n",
        "    return probs_\n",
        "\n",
        "\n",
        "def backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
        "    grads_values = {}\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "\n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
        "\n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "\n",
        "        dA_curr = dA_prev\n",
        "\n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        # b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "    \n",
        "        if activ_function_curr == \"relu\":\n",
        "            backward_activation_func = relu_backward\n",
        "        elif activ_function_curr == \"sigmoid\":\n",
        "            backward_activation_func = sigmoid_backward\n",
        "        elif activ_function_curr == \"linear\":\n",
        "            backward_activation_func = linear_backward\n",
        "        elif activ_function_curr == \"softmax\":    \n",
        "            backward_activation_func = softmax_backward\n",
        "            \n",
        "        dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "        dW_curr = np.dot(dZ_curr, A_prev.T) \n",
        "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) \n",
        "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "\n",
        "    return grads_values\n",
        "\n",
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "    for layer_idx, layer in enumerate(nn_architecture,1):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]\n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return params_values\n",
        "\n",
        "\n",
        "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
        "    params_values = init_layers(nn_architecture,87)\n",
        "    loss_list = []\n",
        "    accuracy_list = []\n",
        "    loss_dict = {}\n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = forward_propagation(X, params_values, nn_architecture)\n",
        "        loss = get_loss_value(Y_hat, Y)\n",
        "        loss_list.append(loss)\n",
        "        accuracy = get_accuracy_value(Y_hat, Y)\n",
        "        accuracy_list.append(accuracy)\n",
        "        loss_dict[i]=loss\n",
        "        if i % 1000 == 0:\n",
        "            print(\"epoch \" ,i,\" loss :\",loss)\n",
        "\n",
        "        grads_values = backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
        "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
        "\n",
        "    return params_values, loss_list, accuracy_list, loss_dict\n",
        "\n",
        "\n",
        "def generate_linear(n=100):\n",
        "    pts = np.random.uniform(0, 1, (n, 2))\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for pt in pts:\n",
        "        inputs.append([pt[0], pt[1]])\n",
        "        # distance = (pt[0] - pt[1]) / 1.414\n",
        "        if pt[0] > pt[1]:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    return np.array(inputs), np.array(labels).reshape(n, 1)\n",
        "\n",
        "\n",
        "def generate_XOR_easy():\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for i in range(11):\n",
        "        inputs.append([0.1 * i, 0.1 * i])\n",
        "        labels.append(0)\n",
        "\n",
        "        if (0.1 * i == 0.5):\n",
        "            continue\n",
        "        inputs.append([0.1 * i, 1 - 0.1 * i])\n",
        "        labels.append(1)\n",
        "    return np.array(inputs), np.array(labels).reshape(21, 1)\n",
        "\n",
        "\n",
        "def show_result(x, y, pred_y):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Ground truth\", fontsize=18)\n",
        "    for i in range(x.shape[0]):\n",
        "        if y[i] == 0:\n",
        "            plt.plot(x[i][0], x[i][1], 'ro')\n",
        "        else:\n",
        "            plt.plot(x[i][0], x[i][1], 'bo')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Predict result\", fontsize=18)\n",
        "    for i in range(x.shape[0]):\n",
        "        if pred_y[i] ==0:\n",
        "            plt.plot(x[i][0], x[i][1], 'ro')\n",
        "        else:\n",
        "            plt.plot(x[i][0], x[i][1], 'bo')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "x,y=generate_linear(n=100)\n",
        "x,y=generate_XOR_easy()\n",
        "\n",
        "#x = xxx.png\n",
        "x = x.reshape(-1)\n",
        "\n",
        "zz,lost,acc,loss_dict = train(x.transpose(),y.transpose(),nn_architecture,20000,0.01)\n",
        "out,_= forward_propagation(x.transpose(),zz,nn_architecture)\n",
        "np.set_printoptions(suppress=True)\n",
        "print(\"Predition:\")\n",
        "print(out)\n",
        "out = convert_prob_into_class(out)\n",
        "show_result(x,y,out.transpose())\n",
        "print(\"Accuracy:\",acc[-1]*100,\"%\")\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Learning Curve\", fontsize=18)\n",
        "plt.plot(list(loss_dict),list(loss_dict.values()))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-522fc9ef9ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0mzz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-522fc9ef9ef7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcashe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-522fc9ef9ef7>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_curr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mactiv_function_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mW_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mb_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'activation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y42CqiC8hAFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06aca48-8dfa-4ca5-b2f7-de80b65cf4f3"
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/train.npz')\n",
        "test_data = np.load('./drive/MyDrive/Colab Notebooks/FNN/test.npz')\n",
        "\n",
        "\n",
        "# Data Pre_process==========================\n",
        "x_train = train_data['image']\n",
        "y_train = train_data['label']\n",
        "\n",
        "x_test = test_data['image']\n",
        "y_test = test_data['label']\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "one_hot_list = [ [1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0], [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]]\n",
        "\n",
        "x_train_row = x_train.reshape(x_train.shape[0] , x_train.shape[1]*x_train.shape[2])\n",
        "y_train_one_hot = []\n",
        "for i in range(len(y_train)):\n",
        "  y_train_one_hot.append( one_hot_list[y_train[i]] )\n",
        "y_train_lab = np.array(y_train_one_hot) \n",
        "\n",
        "x_test_row = x_test.reshape(x_test.shape[0] , x_test.shape[1]*x_test.shape[2])\n",
        "y_test_one_hot = []\n",
        "for i in range(len(y_test)):\n",
        "  y_test_one_hot.append( one_hot_list[y_test[i]] )\n",
        "y_test_lab = np.array(y_test_one_hot) \n",
        "\n",
        "x_train_row = x_train_row / 255\n",
        "x_test_row = x_test_row / 255\n",
        "\n",
        "\n",
        "print(y_train_lab)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(51000, 32, 32)\n",
            "(51000,)\n",
            "(7954, 32, 32)\n",
            "(7954,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po7mjtdXWDaN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}